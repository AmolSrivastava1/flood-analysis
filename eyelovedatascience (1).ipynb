{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixivPb0ML4v0",
        "outputId": "de6fce1f-77b4-4f06-9e8f-5879e742c05b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data successfully retrieved and saved to 'usgs_flood_data.json'\n"
          ]
        }
      ],
      "source": [
        "#initial data import\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# Define the base URL\n",
        "base_url = \"https://stn.wim.usgs.gov/STNServices/Events.json\"\n",
        "\n",
        "# params for data\n",
        "params = {\n",
        "    \"format\": \"json\"  # Ensure the format is JSON\n",
        "}\n",
        "\n",
        "# get request\n",
        "response = requests.get(base_url, params=params)\n",
        "\n",
        "# check response\n",
        "if response.status_code == 200:\n",
        "    data = response.json()\n",
        "\n",
        "    # save data\n",
        "    with open('usgs_flood_data.json', 'w') as outfile:\n",
        "        json.dump(data, outfile, indent=4)\n",
        "\n",
        "    print(\"Data successfully retrieved and saved to 'usgs_flood_data.json'\")\n",
        "else:\n",
        "    print(f\"Failed to retrieve data: {response.status_code}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the content of the JSON file\n",
        "file_path = '/content/usgs_flood_data.json'\n",
        "with open(file_path, 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# display content (first few lines to check for validity)\n",
        "print(content[:500\n",
        "])\n"
      ],
      "metadata": {
        "id": "luCPq8Ehm7i_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Loads the JSON data\n",
        "df = pd.read_json('/content/usgs_flood_data.json')\n",
        "\n",
        "# Remove the specified columns\n",
        "columns_to_remove = [\n",
        "    'event_type_id', 'event_status_id', 'event_coordinator',\n",
        "    'last_updated_by', 'instruments', 'hwms', 'last_updated'\n",
        "]\n",
        "df_cleaned = df.drop(columns=columns_to_remove)\n",
        "\n",
        "# Function to eliminate events with \"Exercise\" or \"Test\" in the event name\n",
        "def remove_exercises_and_tests(df):\n",
        "    return df[~df['event_name'].str.contains('Exercise|Test', case=False)]\n",
        "\n",
        "# Filter the DataFrame for events that started between 2010 and 2020\n",
        "start_date = '2010-01-01'\n",
        "end_date = '2020-12-31'\n",
        "df_filtered = df_cleaned[(df_cleaned['event_start_date'] >= start_date) & (df_cleaned['event_start_date'] <= end_date)]\n",
        "\n",
        "# Remove exercises and tests\n",
        "df_filtered = remove_exercises_and_tests(df_filtered)\n",
        "\n",
        "# Mapping of known hurricane events to their affected states\n",
        "hurricane_states = {\n",
        "    'Irene': ['New York', 'New Jersey', 'Vermont', 'Connecticut', 'Massachusetts'],\n",
        "    'Lee': ['New York', 'New Jersey', 'Pennsylvania'],\n",
        "    'Isaac': ['Louisiana', 'Mississippi', 'Arkansas', 'Missouri', 'Illinois'],\n",
        "    'Sandy': ['New Jersey', 'New York', 'Connecticut', 'Delaware', 'Maryland'],\n",
        "    'Joaquin': ['South Carolina', 'North Carolina'],\n",
        "    'Hermine': ['Florida', 'Georgia', 'South Carolina'],\n",
        "    'Matthew': ['Florida', 'Georgia', 'South Carolina', 'North Carolina'],\n",
        "    'Harvey': ['Texas', 'Louisiana'],\n",
        "    'Irma': ['Florida', 'Georgia', 'South Carolina', 'Alabama'],\n",
        "    'Maria': ['Puerto Rico', 'US Virgin Islands'],\n",
        "    'Jose': ['New Jersey', 'New York', 'Connecticut', 'Massachusetts'],\n",
        "    'Nate': ['Louisiana', 'Mississippi', 'Alabama'],\n",
        "    'Lane': ['Hawaii'],\n",
        "    'Gordon': ['Mississippi', 'Alabama', 'Florida'],\n",
        "    'Florence': ['North Carolina', 'South Carolina'],\n",
        "    'Michael': ['Florida', 'Georgia', 'Alabama'],\n",
        "    'Dorian': ['Florida', 'Georgia', 'South Carolina', 'North Carolina'],\n",
        "    'Isaias': ['Florida', 'North Carolina', 'New York'],\n",
        "    'Laura': ['Louisiana', 'Texas'],\n",
        "    'Sally': ['Florida', 'Alabama'],\n",
        "    'Delta': ['Louisiana', 'Texas']\n",
        "}\n",
        "\n",
        "# Mapping of US regions to their states\n",
        "regions = {\n",
        "    'northeast': ['Connecticut', 'Maine', 'Massachusetts', 'New Hampshire', 'Rhode Island', 'Vermont', 'New Jersey', 'New York', 'Pennsylvania'],\n",
        "    'midwest': ['Illinois', 'Indiana', 'Michigan', 'Ohio', 'Wisconsin', 'Iowa', 'Kansas', 'Minnesota', 'Missouri', 'Nebraska', 'North Dakota', 'South Dakota'],\n",
        "    'south': ['Delaware', 'Florida', 'Georgia', 'Maryland', 'North Carolina', 'South Carolina', 'Virginia', 'West Virginia', 'Alabama', 'Kentucky', 'Mississippi', 'Tennessee', 'Arkansas', 'Louisiana', 'Oklahoma', 'Texas'],\n",
        "    'west': ['Arizona', 'Colorado', 'Idaho', 'Montana', 'Nevada', 'New Mexico', 'Utah', 'Wyoming', 'Alaska', 'California', 'Hawaii', 'Oregon', 'Washington']\n",
        "}\n",
        "\n",
        "# Function to extract states from event name or description\n",
        "def extract_states(row):\n",
        "    state_full_pattern = r'\\b(Alabama|Alaska|Arizona|Arkansas|California|Colorado|Connecticut|Delaware|Florida|Georgia|Hawaii|Idaho|Illinois|Indiana|Iowa|Kansas|Kentucky|Louisiana|Maine|Maryland|Massachusetts|Michigan|Minnesota|Mississippi|Missouri|Montana|Nebraska|Nevada|New Hampshire|New Jersey|New Mexico|New York|North Carolina|North Dakota|Ohio|Oklahoma|Oregon|Pennsylvania|Rhode Island|South Carolina|South Dakota|Tennessee|Texas|Utah|Vermont|Virginia|Washington|West Virginia|Wisconsin|Wyoming)\\b'\n",
        "\n",
        "    state_abbr_pattern = r'\\b(A[LKSZRAEP]|C[AOT]|D[CE]|F[LM]|G[AU]|HI|I[DLNA]|K[SY]|LA|M[DEIAOSNT]|N[CDEHJMVY]|O[HKR]|P[ARW]|RI|S[CD]|T[NX]|UT|V[AIT]|W[AIVY])\\b'\n",
        "\n",
        "    event_name = str(row['event_name']) if row['event_name'] else ''\n",
        "    event_description = str(row['event_description']) if row['event_description'] else ''\n",
        "\n",
        "    state_abbr_matches = re.findall(state_abbr_pattern, event_name + ' ' + event_description)\n",
        "    state_full_matches = re.findall(state_full_pattern, event_name + ' ' + event_description)\n",
        "\n",
        "    all_states = state_abbr_matches + state_full_matches\n",
        "\n",
        "    # Remove duplicates but keep entries with both full state names and abbreviations\n",
        "    state_dict = {}\n",
        "    for state in all_states:\n",
        "        if state in state_full_pattern and state not in state_dict.values():\n",
        "            state_dict[state] = state\n",
        "        elif state in state_abbr_pattern and state not in state_dict.keys():\n",
        "            state_dict[state] = state\n",
        "\n",
        "    # Add hurricane states if the event name matches a known hurricane\n",
        "    for hurricane, states in hurricane_states.items():\n",
        "        if hurricane.lower() in event_name.lower():\n",
        "            for state in states:\n",
        "                if state not in state_dict.values():\n",
        "                    state_dict[state] = state\n",
        "\n",
        "    # Check for regional mentions and add respective states\n",
        "    regions_pattern = '|'.join(regions.keys())\n",
        "    region_matches = re.findall(regions_pattern, event_name + ' ' + event_description, re.IGNORECASE)\n",
        "    for region in region_matches:\n",
        "        region = region.lower()\n",
        "        if region in regions:\n",
        "            for state in regions[region]:\n",
        "                if state not in state_dict.values():\n",
        "                    state_dict[state] = state\n",
        "\n",
        "    if state_dict:\n",
        "        return sorted(state_dict.values())  # Remove duplicates and sort\n",
        "    else:\n",
        "        return ['Null']\n",
        "\n",
        "# Extract states and add as new column\n",
        "df_filtered['state'] = df_filtered.apply(extract_states, axis=1)\n",
        "\n",
        "# Sort the filtered DataFrame by event_start_date\n",
        "df_sorted = df_filtered.sort_values(by='event_start_date')\n",
        "\n",
        "# Function to renumber event IDs in order of date\n",
        "def renumber_event_ids(df):\n",
        "    df = df.reset_index(drop=True)\n",
        "    df['event_id'] = df.index + 1\n",
        "    return df\n",
        "\n",
        "# Renumber event IDs\n",
        "df_sorted = renumber_event_ids(df_sorted)\n",
        "\n",
        "# Display the sorted and renumbered DataFrame\n",
        "print(df_sorted)\n",
        "\n",
        "# Saves final data\n",
        "df_sorted.to_json('/content/usgs_flood_data_cleaned.json', orient='records', lines=True)\n"
      ],
      "metadata": {
        "id": "uN5fMl2AlJbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v650xPXqi9Sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load init data\n",
        "df = pd.read_json('/content/usgs_flood_data.json')\n",
        "\n",
        "# Remove the specified columns\n",
        "columns_to_remove = [\n",
        "    'event_type_id', 'event_status_id', 'event_coordinator',\n",
        "    'last_updated_by', 'instruments', 'hwms', 'last_updated'\n",
        "]\n",
        "df_cleaned = df.drop(columns=columns_to_remove)\n",
        "\n",
        "# Function to eliminate events with \"Exercise\" or \"Test\" in the event name\n",
        "def remove_exercises_and_tests(df):\n",
        "    return df[~df['event_name'].str.contains('Exercise|Test', case=False)]\n",
        "\n",
        "# Filter the DataFrame for events that started between 2010 and 2020\n",
        "start_date = '2010-01-01'\n",
        "end_date = '2020-12-31'\n",
        "df_filtered = df_cleaned[(df_cleaned['event_start_date'] >= start_date) & (df_cleaned['event_start_date'] <= end_date)]\n",
        "\n",
        "# Remove exercises and tests\n",
        "df_filtered = remove_exercises_and_tests(df_filtered)\n",
        "\n",
        "# Mapping of known hurricane events to their affected states\n",
        "hurricane_states = {\n",
        "    'Irene': ['New York', 'New Jersey', 'Vermont', 'Connecticut', 'Massachusetts'],\n",
        "    'Lee': ['New York', 'New Jersey', 'Pennsylvania'],\n",
        "    'Isaac': ['Louisiana', 'Mississippi', 'Arkansas', 'Missouri', 'Illinois'],\n",
        "    'Sandy': ['New Jersey', 'New York', 'Connecticut', 'Delaware', 'Maryland'],\n",
        "    'Joaquin': ['South Carolina', 'North Carolina'],\n",
        "    'Hermine': ['Florida', 'Georgia', 'South Carolina'],\n",
        "    'Matthew': ['Florida', 'Georgia', 'South Carolina', 'North Carolina'],\n",
        "    'Harvey': ['Texas', 'Louisiana'],\n",
        "    'Irma': ['Florida', 'Georgia', 'South Carolina', 'Alabama'],\n",
        "    'Maria': ['Puerto Rico', 'US Virgin Islands'],\n",
        "    'Jose': ['New Jersey', 'New York', 'Connecticut', 'Massachusetts'],\n",
        "    'Nate': ['Louisiana', 'Mississippi', 'Alabama'],\n",
        "    'Lane': ['Hawaii'],\n",
        "    'Gordon': ['Mississippi', 'Alabama', 'Florida'],\n",
        "    'Florence': ['North Carolina', 'South Carolina'],\n",
        "    'Michael': ['Florida', 'Georgia', 'Alabama'],\n",
        "    'Dorian': ['Florida', 'Georgia', 'South Carolina', 'North Carolina'],\n",
        "    'Isaias': ['Florida', 'North Carolina', 'New York'],\n",
        "    'Laura': ['Louisiana', 'Texas'],\n",
        "    'Sally': ['Florida', 'Alabama'],\n",
        "    'Delta': ['Louisiana', 'Texas']\n",
        "}\n",
        "\n",
        "# Mapping of US regions to their states\n",
        "regions = {\n",
        "    'northeast': ['Connecticut', 'Maine', 'Massachusetts', 'New Hampshire', 'Rhode Island', 'Vermont', 'New Jersey', 'New York', 'Pennsylvania'],\n",
        "    'midwest': ['Illinois', 'Indiana', 'Michigan', 'Ohio', 'Wisconsin', 'Iowa', 'Kansas', 'Minnesota', 'Missouri', 'Nebraska', 'North Dakota', 'South Dakota'],\n",
        "    'south': ['Delaware', 'Florida', 'Georgia', 'Maryland', 'North Carolina', 'South Carolina', 'Virginia', 'West Virginia', 'Alabama', 'Kentucky', 'Mississippi', 'Tennessee', 'Arkansas', 'Louisiana', 'Oklahoma', 'Texas'],\n",
        "    'west': ['Arizona', 'Colorado', 'Idaho', 'Montana', 'Nevada', 'New Mexico', 'Utah', 'Wyoming', 'Alaska', 'California', 'Hawaii', 'Oregon', 'Washington']\n",
        "}\n",
        "\n",
        "# Function to extract states from event name or description\n",
        "def extract_states(row):\n",
        "    state_full_pattern = r'\\b(Alabama|Alaska|Arizona|Arkansas|California|Colorado|Connecticut|Delaware|Florida|Georgia|Hawaii|Idaho|Illinois|Indiana|Iowa|Kansas|Kentucky|Louisiana|Maine|Maryland|Massachusetts|Michigan|Minnesota|Mississippi|Missouri|Montana|Nebraska|Nevada|New Hampshire|New Jersey|New Mexico|New York|North Carolina|North Dakota|Ohio|Oklahoma|Oregon|Pennsylvania|Rhode Island|South Carolina|South Dakota|Tennessee|Texas|Utah|Vermont|Virginia|Washington|West Virginia|Wisconsin|Wyoming)\\b'\n",
        "\n",
        "    state_abbr_pattern = r'\\b(A[LKSZRAEP]|C[AOT]|D[CE]|F[LM]|G[AU]|HI|I[DLNA]|K[SY]|LA|M[DEIAOSNT]|N[CDEHJMVY]|O[HKR]|P[ARW]|RI|S[CD]|T[NX]|UT|V[AIT]|W[AIVY])\\b'\n",
        "\n",
        "    event_name = str(row['event_name']) if row['event_name'] else ''\n",
        "    event_description = str(row['event_description']) if row['event_description'] else ''\n",
        "\n",
        "    state_abbr_matches = re.findall(state_abbr_pattern, event_name + ' ' + event_description)\n",
        "    state_full_matches = re.findall(state_full_pattern, event_name + ' ' + event_description)\n",
        "\n",
        "    all_states = state_abbr_matches + state_full_matches\n",
        "\n",
        "    # Remove duplicates but keep entries with both full state names and abbreviations\n",
        "    state_dict = {}\n",
        "    for state in all_states:\n",
        "        if state in state_full_pattern and state not in state_dict.values():\n",
        "            state_dict[state] = state\n",
        "        elif state in state_abbr_pattern and state not in state_dict.keys():\n",
        "            state_dict[state] = state\n",
        "\n",
        "    # Add hurricane states if the event name matches a known hurricane\n",
        "    for hurricane, states in hurricane_states.items():\n",
        "        if hurricane.lower() in event_name.lower():\n",
        "            for state in states:\n",
        "                if state not in state_dict.values():\n",
        "                    state_dict[state] = state\n",
        "\n",
        "    # Checks for regional mentions and add respective states\n",
        "    regions_pattern = '|'.join(regions.keys())\n",
        "    region_matches = re.findall(regions_pattern, event_name + ' ' + event_description, re.IGNORECASE)\n",
        "    for region in region_matches:\n",
        "        region = region.lower()\n",
        "        if region in regions:\n",
        "            for state in regions[region]:\n",
        "                if state not in state_dict.values():\n",
        "                    state_dict[state] = state\n",
        "\n",
        "    if state_dict:\n",
        "        return sorted(state_dict.values())  # Remove duplicates and sort\n",
        "    else:\n",
        "        return ['Null']\n",
        "\n",
        "# Extracts states and add as new column\n",
        "df_filtered['state'] = df_filtered.apply(extract_states, axis=1)\n",
        "\n",
        "# Manually updates inferred states\n",
        "state_updates = {\n",
        "    14: ['Pennsylvania'],\n",
        "    16: ['Pennsylvania'],\n",
        "    27: ['West Virginia'],\n",
        "    23: ['South Carolina'],\n",
        "    36: ['California'],\n",
        "    40: ['Ohio']  # Manually inferred example for event 40\n",
        "}\n",
        "\n",
        "for event_id, states in state_updates.items():\n",
        "    df_filtered.loc[df_filtered['event_id'] == event_id, 'state'] = [states]\n",
        "\n",
        "# Sort the filtered DataFrame by event_start_date\n",
        "df_sorted = df_filtered.sort_values(by='event_start_date')\n",
        "\n",
        "# Function to renumber event IDs in order of date\n",
        "def renumber_event_ids(df):\n",
        "    df = df.reset_index(drop=True)\n",
        "    df['event_id'] = df.index + 1\n",
        "    return df\n",
        "\n",
        "# Renumber event IDs\n",
        "df_sorted = renumber_event_ids(df_sorted)\n",
        "\n",
        "# Display the entries with \"Null\" state after extraction\n",
        "null_state_entries = df_sorted[df_sorted['state'].apply(lambda x: 'Null' in x)]\n",
        "null_state_entries_list = null_state_entries[['event_id', 'event_name', 'event_description', 'event_start_date', 'event_end_date', 'state']]\n",
        "\n",
        "# Display the null state entries list\n",
        "print(null_state_entries_list)\n",
        "\n",
        "# Saves final data\n",
        "df_sorted.to_json('/content/usgs_flood_data_cleaned.json', orient='records', lines=True)\n"
      ],
      "metadata": {
        "id": "UKn_ueo8VcGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Null state checker(change input path of dataset)\n",
        "# Load the JSON data\n",
        "df = pd.read_json('/content/usgs_flood_data_cleaned.json', lines=True)\n",
        "\n",
        "# Total number of events\n",
        "total_events = len(df)\n",
        "\n",
        "# Number of events with 'Null' as the state\n",
        "null_state_events = len(df[df['state'].apply(lambda x: 'Null' in x)])\n",
        "\n",
        "# Percentage of events with 'Null' as the state\n",
        "percentage_null_state_events = round(((null_state_events / total_events) * 100),2)\n",
        "\n",
        "print(\"Floods:\",total_events,\", Floods with no state named:\",null_state_events,\", Percentage unnamed:\",percentage_null_state_events)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LpOe-7-ibiK",
        "outputId": "21c77884-1559-4db3-bfc1-62143260f068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Floods: 65 , Floods with no state named: 16 , Percentage unnamed: 24.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# NOAA API token\n",
        "api_key = 'RdMmjNewLcNoQqMywPAXCCEKutgRAvIr'\n",
        "\n",
        "# Function to get hurricane data from NOAA API\n",
        "def get_hurricane_data(api_key, start_year=2010, end_year=2020):\n",
        "    base_url = \"https://www.ncdc.noaa.gov/cdo-web/api/v2/stormevents/details\"\n",
        "    headers = {\n",
        "        'token': api_key\n",
        "    }\n",
        "    hurricane_states = {}\n",
        "\n",
        "    for year in range(start_year, end_year + 1):\n",
        "        params = {\n",
        "            'datasetid': 'stormevents',\n",
        "            'eventtype': 'Hurricane',\n",
        "            'startdate': f'{year}-01-01',\n",
        "            'enddate': f'{year}-12-31',\n",
        "            'limit': 1000\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(base_url, headers=headers, params=params)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "            for event in data.get('results', []):\n",
        "                event_name = event.get('event_type')\n",
        "                state = event.get('state')\n",
        "                if event_name and state:\n",
        "                    if event_name not in hurricane_states:\n",
        "                        hurricane_states[event_name] = set()\n",
        "                    hurricane_states[event_name].add(state)\n",
        "        except requests.exceptions.HTTPError as http_err:\n",
        "            print(f\"HTTP error occurred for {year}: {http_err}\")\n",
        "        except Exception as err:\n",
        "            print(f\"Other error occurred for {year}: {err}\")\n",
        "\n",
        "    # Convert sets to sorted lists for JSON serialization\n",
        "    for event_name in hurricane_states:\n",
        "        hurricane_states[event_name] = sorted(hurricane_states[event_name])\n",
        "\n",
        "    return hurricane_states\n",
        "\n",
        "# Function to save hurricane data to a JSON file\n",
        "def save_hurricane_data(api_key, start_year=2010, end_year=2020, output_file='hurricane_states.json'):\n",
        "    hurricane_states = get_hurricane_data(api_key, start_year, end_year)\n",
        "    with open(output_file, 'w') as outfile:\n",
        "        json.dump(hurricane_states, outfile, indent=4)\n",
        "    print(f\"Hurricane data successfully saved to '{output_file}'\")\n",
        "\n",
        "# Usage\n",
        "save_hurricane_data(api_key, start_year=2010, end_year=2020)\n"
      ],
      "metadata": {
        "id": "6U7Cj_8_tlTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HERE BEGINS SECTION FOR TWITTER/MONGODB **\n",
        "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
        "⠀⣰⣄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣴⡾\n",
        "⠀⠀⣿⡍⠛⠲⣶⣄⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣠⡴⠞⠉⣠⡞⠀⠀\n",
        "⠀⠀⠘⣽⢷⣦⣌⣈⠋⡚⠿⣦⡀⠀⠀⣴⣶⡄⠀⠀⣠⡶⠚⠛⣙⣭⠠⣤⣶⣯⠆⠀⠀⠀\n",
        "⠀⠀⠀⣼⣷⣀⠀⠀⠈⠀⠀⠀⢻⡇⠺⡿⠛⣿⡅⠀⢿⠀⠀⣼⠿⣫⣭⣠⣤⡶⠂⠀⠀⠀\n",
        "⠀⠀⠀⠀⠉⠛⠿⣹⣾⠔⠃⠀⠈⠳⠾⠏⠀⠻⣷⡺⠋⠀⣤⣸⣷⣶⡾⠖⠀⠀⠀⠀⠀⠀\n",
        "⠀⠀⠀⠀⠀⠈⠒⠷⣿⡻⣞⣀⣄⣀⣀⡄⠀⠀⣠⣄⣸⡿⣾⣿⡽⡄⠀⠀⠀⠀⠀⠀⠀⠀\n",
        "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⠟⠯⣽⢿⡿⠃⠀⢀⣿⡙⠑⠙⠛⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
        "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⣯⣦⣾⣿⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
        "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣼⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
        "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣿⢩⡿⠘⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
        "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣽⡃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
        "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀"
      ],
      "metadata": {
        "id": "xnF7FwnwjNM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Necessary(or not) installs\n",
        "!pip install pymongo\n",
        "!pip install textblob\n",
        "!pip install tweepy\n",
        "!pip install pandas\n",
        "!pip install re\n",
        "!pip install pymongo[srv] tweepy textblob\n",
        "!pip install --upgrade pymongo[srv]\n",
        "!pip install geopy\n",
        "!pip install Twarc2\n",
        "!pip install twarc\n",
        "!pip install certifi\n",
        "!pip install pytrends pymongo[srv] certifi\n",
        "!pip install pytrends pymongo[srv] certifi\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install pymongo[srv]\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mnWgyUY7i_N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "\n",
        "uri = \"mongodb+srv://cjp224:N4IR3nyeoqOak7yD@cs210data.5ghkb6u.mongodb.net/?retryWrites=true&w=majority&appName=CS210Data\"\n",
        "\n",
        "# Create a new client and connect to the server\n",
        "client = MongoClient(uri, server_api=ServerApi('1'), tls=True, tlsAllowInvalidCertificates=True)\n",
        "\n",
        "# Send a ping to confirm a successful connection\n",
        "try:\n",
        "    client.admin.command('ping')\n",
        "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
        "except Exception as e:\n",
        "    print(e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5W6N2UhRAx9K",
        "outputId": "785a1a52-3b1c-4058-edc4-fbdf937db86c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SSL handshake failed: ac-l21diot-shard-00-02.5ghkb6u.mongodb.net:27017: [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1007) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms),SSL handshake failed: ac-l21diot-shard-00-00.5ghkb6u.mongodb.net:27017: [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1007) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms),SSL handshake failed: ac-l21diot-shard-00-01.5ghkb6u.mongodb.net:27017: [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1007) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms), Timeout: 30s, Topology Description: <TopologyDescription id: 66980147109bd788de7122bb, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('ac-l21diot-shard-00-00.5ghkb6u.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('SSL handshake failed: ac-l21diot-shard-00-00.5ghkb6u.mongodb.net:27017: [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1007) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>, <ServerDescription ('ac-l21diot-shard-00-01.5ghkb6u.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('SSL handshake failed: ac-l21diot-shard-00-01.5ghkb6u.mongodb.net:27017: [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1007) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>, <ServerDescription ('ac-l21diot-shard-00-02.5ghkb6u.mongodb.net', 27017) server_type: Unknown, rtt: None, error=AutoReconnect('SSL handshake failed: ac-l21diot-shard-00-02.5ghkb6u.mongodb.net:27017: [SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1007) (configured timeouts: socketTimeoutMS: 20000.0ms, connectTimeoutMS: 20000.0ms)')>]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Initial twitter attempt via tweepy\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "import certifi\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.exc import GeocoderTimedOut\n",
        "\n",
        "# Twitter API v2 setup\n",
        "consumer_key = 'WcbUX1tcq66sHTNdDn0FtdUvk'\n",
        "consumer_secret = 'JJYd7xLHDE4qYnljrGrIIGXaeemOFIPRivJGhaT2AYrQI2Hdkc'\n",
        "access_token = '1803477174655262722-yTnE5eqoVjKWmRV1svObxL9kByjJZA'\n",
        "access_token_secret = 'sw4xZINk9pihuWnxjHSRlwO4X5t25q91yh1wBs0D2gx25'\n",
        "bearer_token = 'AAAAAAAAAAAAAAAAAAAAAB8xuwEAAAAAQXktnZ8i826qQoSk7L32Jyxtqoc%3DmKdq8lt7PsjWId6p2KJoX57paC6KUITrWzsQzRgMwigQrRBypN'\n",
        "\n",
        "client = tweepy.Client(bearer_token=bearer_token,\n",
        "                       consumer_key=consumer_key,\n",
        "                       consumer_secret=consumer_secret,\n",
        "                       access_token=access_token,\n",
        "                       access_token_secret=access_token_secret)\n",
        "\n",
        "# Define search query without place_country operator\n",
        "search_query = \"flood -is:retweet -is:reply\"\n",
        "no_of_tweets = 10\n",
        "\n",
        "# Geolocator setup\n",
        "geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
        "\n",
        "# Function to get state from location\n",
        "def get_state_from_location(lat, lon):\n",
        "    try:\n",
        "        location = geolocator.reverse((lat, lon), timeout=10)\n",
        "        if location and 'address' in location.raw:\n",
        "            address = location.raw['address']\n",
        "            return address.get('state', 'Unknown')\n",
        "    except GeocoderTimedOut:\n",
        "        return 'Unknown'\n",
        "    return 'Unknown'\n",
        "\n",
        "try:\n",
        "    response = client.search_recent_tweets(query=search_query, max_results=no_of_tweets, tweet_fields=['created_at', 'public_metrics', 'source', 'text', 'geo'])\n",
        "    tweets = response.data\n",
        "\n",
        "    if tweets:\n",
        "        # Extract tweet attributes and create DataFrame\n",
        "        attributes_container = []\n",
        "        for tweet in tweets:\n",
        "            if tweet.geo and 'coordinates' in tweet.geo:\n",
        "                lat, lon = tweet.geo['coordinates']['coordinates']\n",
        "                state = get_state_from_location(lat, lon)\n",
        "            else:\n",
        "                state = 'Unknown'\n",
        "            attributes_container.append([tweet.author_id, tweet.created_at, tweet.public_metrics['like_count'], tweet.source, tweet.text, state])\n",
        "\n",
        "        columns = [\"User\", \"Date Created\", \"Number of Likes\", \"Source of Tweet\", \"Tweet\", \"State\"]\n",
        "        tweets_df = pd.DataFrame(attributes_container, columns=columns)\n",
        "        print(\"Tweets DataFrame created successfully.\")\n",
        "    else:\n",
        "        print(\"No tweets found.\")\n",
        "        tweets_df = pd.DataFrame(columns=[\"User\", \"Date Created\", \"Number of Likes\", \"Source of Tweet\", \"Tweet\", \"State\"])\n",
        "\n",
        "except tweepy.TweepyException as e:\n",
        "    print('Status Failed On,', str(e))\n",
        "    tweets_df = pd.DataFrame(columns=[\"User\", \"Date Created\", \"Number of Likes\", \"Source of Tweet\", \"Tweet\", \"State\"])\n",
        "\n",
        "# Ensure tweets_df has data\n",
        "print(f\"Number of tweets fetched: {len(tweets_df)}\")\n",
        "\n",
        "if not tweets_df.empty:\n",
        "    # Filter tweets from 2010 to 2020\n",
        "    start_date = datetime(2010, 1, 1, tzinfo=pytz.UTC)\n",
        "    end_date = datetime(2020, 12, 31, tzinfo=pytz.UTC)\n",
        "    tweets_df['Date Created'] = pd.to_datetime(tweets_df['Date Created'])\n",
        "    tweets_df = tweets_df[(tweets_df['Date Created'] >= start_date) & (tweets_df['Date Created'] <= end_date)]\n",
        "\n",
        "    print(f\"Number of tweets after date filter: {len(tweets_df)}\")\n",
        "\n",
        "    # Clean tweet text\n",
        "    def clean_tweet(text):\n",
        "        text = re.sub(r'http\\S+', '', text)\n",
        "        text = re.sub(r'@\\S+', '', text)\n",
        "        text = re.sub(r'#\\S+', '', text)\n",
        "        return text\n",
        "\n",
        "    tweets_df['Cleaned Tweet'] = tweets_df['Tweet'].apply(clean_tweet)\n",
        "\n",
        "    print(f\"Sample cleaned tweets: {tweets_df['Cleaned Tweet'].head()}\")\n",
        "\n",
        "    # Sentiment Analysis\n",
        "    def analyze_sentiment(text):\n",
        "        analysis = TextBlob(text)\n",
        "        return 'positive' if analysis.sentiment.polarity > 0 else 'negative' if analysis.sentiment.polarity < 0 else 'neutral'\n",
        "\n",
        "    tweets_df['Sentiment'] = tweets_df['Cleaned Tweet'].apply(analyze_sentiment)\n",
        "\n",
        "    # Tokenization\n",
        "    tweets_df['Tokens'] = tweets_df['Cleaned Tweet'].apply(lambda x: x.split())\n",
        "\n",
        "    # Custom filtering for contextually relevant tweets\n",
        "    def is_relevant_tweet(text):\n",
        "        relevant_keywords = ['flood', 'flooding', 'flooded', 'floods', 'rain', 'river', 'storm', 'disaster']\n",
        "        irrelevant_phrases = ['my inbox', 'my emails', 'my email', 'my phone', 'workload', 'work load', 'calendar']\n",
        "\n",
        "        for phrase in irrelevant_phrases:\n",
        "            if phrase in text.lower():\n",
        "                return False\n",
        "\n",
        "        return any(keyword in text.lower() for keyword in relevant_keywords)\n",
        "\n",
        "    tweets_df = tweets_df[tweets_df['Cleaned Tweet'].apply(is_relevant_tweet)]\n",
        "\n",
        "    print(f\"Number of relevant tweets: {len(tweets_df)}\")\n",
        "\n",
        "    # MongoDB Atlas setup\n",
        "    uri = \"mongodb+srv://cjp224:N4IR3nyeoqOak7yD@cs210data.5ghkb6u.mongodb.net/?retryWrites=true&w=majority&appName=CS210Data\"\n",
        "    mongo_client = MongoClient(uri, server_api=ServerApi('1'), tls=True, tlsCAFile=certifi.where())\n",
        "\n",
        "    try:\n",
        "        mongo_client.admin.command('ping')\n",
        "        print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
        "\n",
        "        db = mongo_client['CS210Data']\n",
        "        tweets_collection = db['tweets']\n",
        "\n",
        "        # Insert tweets into MongoDB\n",
        "        tweets_data = tweets_df.to_dict('records')\n",
        "        if tweets_data:\n",
        "            tweets_collection.insert_many(tweets_data)\n",
        "            print(\"Tweets inserted into MongoDB successfully.\")\n",
        "\n",
        "            # Retrieve sorted tweets from MongoDB\n",
        "            sorted_tweets = tweets_collection.find().sort('Date Created', -1)\n",
        "            tweets_data_sorted = list(sorted_tweets)\n",
        "            df_tweets_sorted = pd.DataFrame(tweets_data_sorted)\n",
        "\n",
        "            # Load cleaned flood data\n",
        "            df_flood = pd.read_json('/mnt/data/usgs_flood_data_cleaned.json')\n",
        "\n",
        "            # Example comparison: Number of tweets per flood event\n",
        "            df_flood['Tweet Count'] = df_flood['event_name'].apply(lambda x: df_tweets_sorted['Tweet'].str.contains(x, case=False).sum())\n",
        "\n",
        "            print(df_flood[['event_name', 'Tweet Count']])\n",
        "        else:\n",
        "            print(\"No valid tweet data to insert into MongoDB.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Could not connect to MongoDB:\", e)\n",
        "else:\n",
        "    print(\"tweets_df is empty. No tweets to process.\")\n"
      ],
      "metadata": {
        "id": "mDRZvvRYD4Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Secondary Twitter/X attempt via twarc2\n",
        "from twarc import Twarc2, expansions\n",
        "import datetime\n",
        "import json\n",
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "import certifi\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.exc import GeocoderTimedOut\n",
        "\n",
        "# Replace your bearer token below\n",
        "client = Twarc2(bearer_token=\"AAAAAAAAAAAAAAAAAAAAAB8xuwEAAAAAQXktnZ8i826qQoSk7L32Jyxtqoc%3DmKdq8lt7PsjWId6p2KJoX57paC6KUITrWzsQzRgMwigQrRBypN\")\n",
        "\n",
        "# MongoDB Atlas setup\n",
        "uri = \"mongodb+srv://cjp224:N4IR3nyeoqOak7yD@cs210data.5ghkb6u.mongodb.net/?retryWrites=true&w=majority&appName=CS210Data\"\n",
        "mongo_client = MongoClient(uri, server_api=ServerApi('1'), tls=True, tlsCAFile=certifi.where())\n",
        "\n",
        "try:\n",
        "    mongo_client.admin.command('ping')\n",
        "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
        "except Exception as e:\n",
        "    print(\"Could not connect to MongoDB:\", e)\n",
        "\n",
        "# Geolocator setup\n",
        "geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
        "\n",
        "# Function to get state from location\n",
        "def get_state_from_location(lat, lon):\n",
        "    try:\n",
        "        location = geolocator.reverse((lat, lon), timeout=10)\n",
        "        if location and 'address' in location.raw:\n",
        "            address = location.raw['address']\n",
        "            return address.get('state', 'Unknown')\n",
        "    except GeocoderTimedOut:\n",
        "        return 'Unknown'\n",
        "    return 'Unknown'\n",
        "\n",
        "# Function to fetch tweets for a given year\n",
        "def fetch_tweets_for_year(year):\n",
        "    start_time = datetime.datetime(year, 1, 1, 0, 0, 0, 0, datetime.timezone.utc)\n",
        "    end_time = datetime.datetime(year, 12, 31, 23, 59, 59, 999999, datetime.timezone.utc)\n",
        "    query = \"flood -is:retweet -is:reply\"\n",
        "\n",
        "    search_results = client.search_all(query=query, start_time=start_time, end_time=end_time, max_results=10)\n",
        "\n",
        "    tweets_data = []\n",
        "    for page in search_results:\n",
        "        result = expansions.flatten(page)\n",
        "        for tweet in result:\n",
        "            if 'geo' in tweet and tweet['geo']:\n",
        "                lat, lon = tweet['geo']['coordinates']['coordinates']\n",
        "                state = get_state_from_location(lat, lon)\n",
        "            else:\n",
        "                state = 'Unknown'\n",
        "            tweets_data.append({\n",
        "                'User': tweet['author_id'],\n",
        "                'Date Created': tweet['created_at'],\n",
        "                'Number of Likes': tweet['public_metrics']['like_count'],\n",
        "                'Source of Tweet': tweet['source'],\n",
        "                'Tweet': tweet['text'],\n",
        "                'State': state\n",
        "            })\n",
        "\n",
        "    return tweets_data\n",
        "\n",
        "# Fetch tweets from 2010 to 2020\n",
        "all_tweets = []\n",
        "for year in range(2010, 2021):\n",
        "    print(f\"Fetching tweets for {year}...\")\n",
        "    tweets_data = fetch_tweets_for_year(year)\n",
        "    all_tweets.extend(tweets_data)\n",
        "    print(f\"Number of tweets fetched for {year}: {len(tweets_data)}\")\n",
        "\n",
        "# Create DataFrame from tweets\n",
        "tweets_df = pd.DataFrame(all_tweets)\n",
        "print(f\"Total number of tweets fetched: {len(tweets_df)}\")\n",
        "\n",
        "if not tweets_df.empty:\n",
        "    # Clean tweet text\n",
        "    def clean_tweet(text):\n",
        "        text = re.sub(r'http\\S+', '', text)\n",
        "        text = re.sub(r'@\\S+', '', text)\n",
        "        text = re.sub(r'#\\S+', '', text)\n",
        "        return text\n",
        "\n",
        "    tweets_df['Cleaned Tweet'] = tweets_df['Tweet'].apply(clean_tweet)\n",
        "\n",
        "    # Sentiment Analysis\n",
        "    def analyze_sentiment(text):\n",
        "        analysis = TextBlob(text)\n",
        "        return 'positive' if analysis.sentiment.polarity > 0 else 'negative' if analysis.sentiment.polarity < 0 else 'neutral'\n",
        "\n",
        "    tweets_df['Sentiment'] = tweets_df['Cleaned Tweet'].apply(analyze_sentiment)\n",
        "\n",
        "    # Tokenization\n",
        "    tweets_df['Tokens'] = tweets_df['Cleaned Tweet'].apply(lambda x: x.split())\n",
        "\n",
        "    # Custom filtering for contextually relevant tweets\n",
        "    def is_relevant_tweet(text):\n",
        "        relevant_keywords = ['flood', 'flooding', 'flooded', 'floods', 'rain', 'river', 'storm', 'disaster']\n",
        "        irrelevant_phrases = ['my inbox', 'my emails', 'my email', 'my phone', 'workload', 'work load', 'calendar']\n",
        "\n",
        "        for phrase in irrelevant_phrases:\n",
        "            if phrase in text.lower():\n",
        "                return False\n",
        "\n",
        "        return any(keyword in text.lower() for keyword in relevant_keywords)\n",
        "\n",
        "    tweets_df = tweets_df[tweets_df['Cleaned Tweet'].apply(is_relevant_tweet)]\n",
        "\n",
        "    print(f\"Number of relevant tweets: {len(tweets_df)}\")\n",
        "\n",
        "    db = mongo_client['CS210Data']\n",
        "    tweets_collection = db['tweets']\n",
        "\n",
        "    # Insert tweets into MongoDB\n",
        "    tweets_data = tweets_df.to_dict('records')\n",
        "    if tweets_data:\n",
        "        tweets_collection.insert_many(tweets_data)\n",
        "        print(\"Tweets inserted into MongoDB successfully.\")\n",
        "\n",
        "        # Retrieve sorted tweets from MongoDB\n",
        "        sorted_tweets = tweets_collection.find().sort('Date Created', -1)\n",
        "        tweets_data_sorted = list(sorted_tweets)\n",
        "        df_tweets_sorted = pd.DataFrame(tweets_data_sorted)\n",
        "\n",
        "        # Load cleaned flood data\n",
        "        df_flood = pd.read_json('/mnt/data/usgs_flood_data_cleaned.json')\n",
        "\n",
        "        # Example comparison: Number of tweets per flood event\n",
        "        df_flood['Tweet Count'] = df_flood['event_name'].apply(lambda x: df_tweets_sorted['Tweet'].str.contains(x, case=False).sum())\n",
        "\n",
        "        print(df_flood[['event_name', 'Tweet Count']])\n",
        "    else:\n",
        "        print(\"No valid tweet data to insert into MongoDB.\")\n",
        "else:\n",
        "    print(\"tweets_df is empty. No tweets to process.\")\n"
      ],
      "metadata": {
        "id": "E80enySR3f22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Initial Google Trends attempt via their API\n",
        "!pip install pytrends pymongo[srv] certifi matplotlib seaborn\n",
        "\n",
        "from pytrends.request import TrendReq\n",
        "import pandas as pd\n",
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "import certifi\n",
        "from datetime import datetime\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# MongoDB connection setup\n",
        "uri = \"mongodb+srv://cjp224:N4IR3nyeoqOak7yD@cs210data.5ghkb6u.mongodb.net/?retryWrites=true&w=majority&appName=CS210Data\"\n",
        "client = MongoClient(uri, server_api=ServerApi('1'), tls=True, tlsCAFile=certifi.where())\n",
        "try:\n",
        "    client.admin.command('ping')\n",
        "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
        "except Exception as e:\n",
        "    print(\"Could not connect to MongoDB:\", e)\n",
        "\n",
        "# Fetch Google Trends data\n",
        "pytrends = TrendReq(hl='en-US', tz=360)\n",
        "\n",
        "def fetch_google_trends_data(keyword, start_year, end_year, max_points=10000):\n",
        "    all_data = []\n",
        "    points_fetched = 0\n",
        "    backoff_time = 1  # Initial backoff time in seconds\n",
        "    for year in range(start_year, end_year + 1):\n",
        "        for month in range(1, 13):\n",
        "            if points_fetched >= max_points:\n",
        "                break\n",
        "            timeframe = f\"{year}-{month:02d}-01 {year}-{month:02d}-28\"\n",
        "            try:\n",
        "                pytrends.build_payload([keyword], cat=0, timeframe=timeframe, geo='US', gprop='')\n",
        "                interest_over_time_df = pytrends.interest_over_time()\n",
        "                if not interest_over_time_df.empty:\n",
        "                    interest_over_time_df.reset_index(inplace=True)\n",
        "                    interest_over_time_df['year'] = year\n",
        "                    interest_over_time_df['month'] = month\n",
        "                    all_data.append(interest_over_time_df)\n",
        "                    points_fetched += len(interest_over_time_df)\n",
        "                time.sleep(2)  # Add a delay between requests\n",
        "                backoff_time = 1  # Reset backoff time after a successful request\n",
        "            except Exception as e:\n",
        "                print(f\"Error fetching data for {timeframe}: {e}\")\n",
        "                print(f\"Sleeping for {backoff_time} seconds before retrying...\")\n",
        "                time.sleep(backoff_time)\n",
        "                backoff_time = min(backoff_time * 2, 60)  # Exponential backoff with a cap at 60 seconds\n",
        "        if points_fetched >= max_points:\n",
        "            break\n",
        "    return pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "flood_trends_df = fetch_google_trends_data('flood', 2010, 2020)\n",
        "flood_trends_df['date'] = flood_trends_df['date'].dt.to_period('M')\n",
        "monthly_interest = flood_trends_df.groupby(['date'])['flood'].mean().reset_index()\n",
        "\n",
        "# Load USGS flood data\n",
        "usgs_flood_df = pd.read_json('/content/usgs_flood_data_cleaned.json', lines=True)\n",
        "usgs_flood_df['event_start_date'] = pd.to_datetime(usgs_flood_df['event_start_date'])\n",
        "usgs_flood_df['year'] = usgs_flood_df['event_start_date'].dt.year\n",
        "usgs_flood_df['month'] = usgs_flood_df['event_start_date'].dt.month\n",
        "\n",
        "# Merge datasets\n",
        "merged_df = pd.merge(usgs_flood_df, monthly_interest, left_on=['year', 'month'], right_on=['date'])\n",
        "\n",
        "# Compare Trends and Events\n",
        "def compare_trends_and_events(df):\n",
        "    comparison_result = df.groupby(['state'])['flood'].mean().reset_index()\n",
        "    return comparison_result\n",
        "\n",
        "comparison_result = compare_trends_and_events(merged_df)\n",
        "print(comparison_result)\n",
        "\n",
        "# Store Data in MongoDB\n",
        "try:\n",
        "    db = client['CS210Data']\n",
        "    trends_collection = db['google_trends']\n",
        "    trends_data = merged_df.to_dict('records')\n",
        "    if trends_data:\n",
        "        trends_collection.insert_many(trends_data)\n",
        "        print(\"Trends data inserted into MongoDB successfully.\")\n",
        "    else:\n",
        "        print(\"No valid trend data to insert into MongoDB.\")\n",
        "except Exception as e:\n",
        "    print(\"Could not connect to MongoDB:\", e)\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.lineplot(data=monthly_interest, x='date', y='flood')\n",
        "plt.title('Google Trends Interest Over Time for \"Flood\" (2010-2020)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Interest')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=comparison_result, x='state', y='flood')\n",
        "plt.title('Average Google Trends Interest by State')\n",
        "plt.xlabel('State')\n",
        "plt.ylabel('Average Interest')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "_OLUBa6b9Yvf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}